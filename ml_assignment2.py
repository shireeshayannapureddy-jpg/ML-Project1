# -*- coding: utf-8 -*-
"""ML-Assignment2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11AxOnpPN30cHDIwGszehy54j-8KG6_RR
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef
df = pd.read_csv("Titanic-Dataset.csv")
df.head()

df.describe()
df.info()

df.isnull().sum()

df['Age'].fillna(df['Age'].mean(), inplace=True)
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)
df['Cabin_Known'] = df['Cabin'].notnull().astype(int)
df.drop('Cabin', axis=1, inplace=True)

df.isnull().sum()

sns.countplot(
    x='Cabin_Known',
    hue='Survived',
    data=df
)

plt.title("Survival vs Cabin Availability")
plt.xlabel("Cabin Availability (0 = Not Available, 1 = Available)")
plt.ylabel("Number of Passengers")
plt.legend(title="Survival Status", labels=["Did Not Survive", "Survived"])
plt.show()

df['Age_Group'] = pd.cut(
    df['Age'],
    bins=[0,12,20,40,60,80],
    labels=['Child','Teen','Adult','Middle-Aged','Senior']
)
sns.countplot(x='Age_Group', hue='Survived', data=df)
plt.title("Survival by Age Group")
plt.show()

df['Fare_Group'] = pd.qcut(df['Fare'], 4, labels=['Low','Medium','High','Very High'])
sns.countplot(x='Fare_Group', hue='Survived', data=df)
plt.title("Survival by Fare Group")
plt.show()

sns.catplot(
    x='Pclass',
    hue='Survived',
    col='Sex',
    data=df,
    kind='count'
)
plt.show()

sns.countplot(x='Embarked', hue='Pclass', data=df)
plt.title("Passenger Class Distribution by Port")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# Encode categorical columns (if not already encoded)
le = LabelEncoder()
df['Sex'] = le.fit_transform(df['Sex'])
df['Embarked'] = le.fit_transform(df['Embarked'])

# Select only numeric columns
df_numeric = df.select_dtypes(include=['int64', 'float64'])

# Compute correlation matrix
corr_matrix = df_numeric.corr()

# Plot heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(
    corr_matrix,
    annot=True,
    cmap='coolwarm',
    fmt=".2f",
    linewidths=0.5
)
plt.title("Correlation Heatmap of Titanic Features")
plt.show()

selected_features = [
    'Sex',
    'Pclass',
    'Fare',
    'Cabin_Known',
    'Age',
    'Embarked',
    'Survived'
]

df_final = df[selected_features]
X = df_final.drop('Survived', axis=1)
y = df_final['Survived']
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train[['Age', 'Fare']] = scaler.fit_transform(
    X_train[['Age', 'Fare']]
)

X_test[['Age', 'Fare']] = scaler.transform(
    X_test[['Age', 'Fare']]
)
from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train, y_train)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    matthews_corrcoef
)

import pandas as pd
from sklearn.metrics import accuracy_score, classification_report

y_pred = lr_model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    matthews_corrcoef
)

import pandas as pd
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "Naive Bayes": GaussianNB(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss')
}
results = []

for model_name, model in models.items():
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    results.append({
        "Model": model_name,
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred),
        "AUC": roc_auc_score(y_test, y_prob) if y_prob is not None else "NA",
        "MCC": matthews_corrcoef(y_test, y_pred)
    })
results_df = pd.DataFrame(results)
results_df

